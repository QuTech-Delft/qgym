{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c91bb9",
   "metadata": {},
   "source": [
    "# Maze runner\n",
    "In this notebook we will cover the basics of a reinforcement learning (RL) environment.\n",
    "\n",
    "Specifically, we will cover the observation, action, and state space following the example of a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from qgym.spaces import Discrete\n",
    "from qgym.templates import Environment, Rewarder, State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613c7f6",
   "metadata": {},
   "source": [
    "### Maze layout\n",
    "\n",
    "In this notebook we will design our own maze, which the agent will learn to navigate through.\n",
    "This maze will have 4 different field types.\n",
    "\n",
    "- `F`: a free field\n",
    "- `W`: a wall\n",
    "- `G`: the goal\n",
    "- `S`: possible start position(s); also a free field\n",
    "\n",
    "### To Do\n",
    "Design a fun 4 by 4 maze by making a list of 4 letter strings. For example `[\"FSFF\", \"SWFW\", \"FFFW\", \"WFFG\"]` would be a fine maze.\n",
    "Note that we allow to have multiple starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_map_4x4 = []  # Make your own maze\n",
    "print(\"\\n\".join(maze_map_4x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9c75",
   "metadata": {},
   "source": [
    "### Environment spaces\n",
    "\n",
    "A Reinforcement Learning environment consists of several spaces that describe its workings:\n",
    "- State space\n",
    "- Action space\n",
    "- Observation space\n",
    "\n",
    "#### State space\n",
    "The state space contains the complete information of the current state of the environment.\n",
    "The state space is for internal use and is not used by the agent.\n",
    "\n",
    "In our maze environment the state consists of five items:\n",
    "- `position`: the current position of the agent in the maze\n",
    "- `maze_map`: a map of the maze\n",
    "- `start_position_distribution`: Distribution from which position an episode can start.\n",
    "- `nrows`: Number of rows in the maze grid.\n",
    "- `ncols`: Number of collumns in the maze grid.\n",
    "\n",
    "#### Action space\n",
    "The action space describes all potential actions that an agent can take in this environment.\n",
    "\n",
    "In our maze environment there are only 4 such actions:\n",
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT\n",
    "\n",
    "#### Observation space\n",
    "The observation space describes all potential observations that an agent can obtain from the environment. It is generally a subset (or a transformation thereof) of the state space.\n",
    "\n",
    "In our case the observation space contains all possible `position`s the agent can be at.\n",
    "\n",
    "#### Constraints on the spaces\n",
    "Reinforcement Learning agents often accept a limited amount of data types.\n",
    "The most commonly supported data types are `int`, `float`, `char` and arrays of these types.\n",
    "Therefore, we will transform the fun maze above into such an array.\n",
    "Furthermore, when setting up the environment, we will need a starting position.\n",
    "Since we allow for multiple starting positions, we should determine a probability distribution to say where we start in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae136aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can transform our map to a 4x4 array of chars as follows:\n",
    "transformed_maze = np.asarray(maze_map_4x4, dtype=\"c\")\n",
    "print(f\"Maze:\\n{transformed_maze}\\n\")\n",
    "\n",
    "# we can compute a probability distribution over the start positions as follows\n",
    "distribution = (transformed_maze == b\"S\").ravel().astype(\"float64\")\n",
    "distribution /= distribution.sum()\n",
    "print(f\"Distribution:\\n{distribution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd2261f",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Setup state space by setting up the attributes `position`, `maze_map`, `start_position_distribution`, `nrows` and `ncols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(State):\n",
    "    def __init__(self, maze_map):\n",
    "        maze_map = np.asarray(maze_map, dtype=\"c\")\n",
    "\n",
    "        self.start_position_distribution = (maze_map == b\"S\").ravel().astype(float)\n",
    "        self.start_position_distribution /= self.start_position_distribution.sum()\n",
    "\n",
    "        # Set the other internal attributes\n",
    "        # self.nrows =\n",
    "        # self.ncols =\n",
    "        # self.position =\n",
    "        # self.maze_map ="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca486c97",
   "metadata": {},
   "source": [
    "### Initial position\n",
    "At the beginning of each episode, the environment should provide the agent with a fresh start, without any leftovers over the previous iteration.\n",
    "\n",
    "In this environment, the fresh start consists of a randomly selected initial position from all possible _start positions_.\n",
    "\n",
    "### To Do\n",
    "First implement two utility methods `rowcol_to_pos` and `pos_to_rowcol`.\n",
    "Subsequently, implement the `reset` method (for which you will need the `pos_to_rowcol` method).\n",
    "\n",
    "_Hint: Each environment has a random number generator `self.rng` with a [`choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) method._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f1f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(MazeState):\n",
    "    def rowcol_to_pos(self, row, col):\n",
    "        # todo: transform (row, col) pair to an integral position\n",
    "\n",
    "    def pos_to_rowcol(self, pos):\n",
    "        # todo: inverse transformation\n",
    "\n",
    "    def reset(self, *, seed=None, return_info=False):\n",
    "        # todo: reset the position to a new random one\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaef3d",
   "metadata": {},
   "source": [
    "### First steps\n",
    "In order to let the agent actually get out of the maze it needs to be able to move to an adjacent field. Recall that we defined our action space as:\n",
    "\n",
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT.\n",
    "\n",
    "### To Do\n",
    "Write the `update_state` method, which should update the state according to the given action.\n",
    "\n",
    "_Note: How should we deal with illegal actions (i.e. falling of the grid, bumping into a wall)?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(MazeState):\n",
    "    def update_state(self, action):\n",
    "        # todo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133121dc",
   "metadata": {},
   "source": [
    "### Observational awareness\n",
    "Our environment is nearly done, with the `update_state` method the agent can move through the maze.\n",
    "However, remember that the agent can't see the state space!\n",
    "We still need to provide our agent a set of 'eyes', i.e., observations.\n",
    "\n",
    "Specifically we need to inform the agent of 2 more things:\n",
    "\n",
    "- The current position (our observation).\n",
    "- Whether we have reached the exit (are we done?).\n",
    "\n",
    "### To Do\n",
    "Implement the `create_observation_space`, `obtain_observation` and `is_done` methods.\n",
    "\n",
    "_Hint: `qgym` provides a ready-to-use `Discrete` action/observation space._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Maze):\n",
    "    def create_observation_space(self):\n",
    "        # todo\n",
    "    \n",
    "    def _obtain_observation(self):\n",
    "        # todo\n",
    "\n",
    "    def _is_done(self):\n",
    "        # todo\n",
    "\n",
    "    ### This method is given 'glue code' ###\n",
    "    def _obtain_info(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4008e",
   "metadata": {},
   "source": [
    "### Carrot and Stick\n",
    "The final step towards completing our `Maze` environment is about giving feedback. We can give 3 types of feedback:\n",
    "- positive feedback (carrot)\n",
    "- negative feedback (stick)\n",
    "- neutral feedback\n",
    "\n",
    "In Reinforcement Learning, feedback is given by means of rewards. The choice of rewarder function has a lot of influence on the learning ability of the agent.\n",
    "\n",
    "Only providing rewards might lead to slow training and too much exploration. However, big penalties could make the agent skip exploration and stick to a safe, potentially non-optimal path.\n",
    "\n",
    "Below is room for two the implementation rewarders `CarrotOnly` (1) and `CarrotAndSticks` (2):\n",
    "\n",
    "1. Provides a positive reward only when the goal is reached. Does nothing otherwise\n",
    "2. Provides a positive reward when the goal is reached. Gives a negative reward (penalty) when the agent bumps into a wall. Otherwise, nothing.\n",
    "\n",
    "We have also including code blocks for training an agent on the `Maze` environment with either rewarder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ce82a",
   "metadata": {},
   "source": [
    "### To Do\n",
    "#### Carrot only\n",
    "In the first block below we can implement the first rewarder:\n",
    "\n",
    "1. Provides a positive reward only when the goal is reached. Does nothing otherwise\n",
    "\n",
    "After defining the rewarder we can train an agent by running the second code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrotOnly(Rewarder):\n",
    "    def __init__(self):\n",
    "        # todo\n",
    "        # self._reward_range = (0, 0)\n",
    "\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        # todo: return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment and rewarder\n",
    "env = Maze(maze_map_4x4)\n",
    "env.rewarder = CarrotOnly()\n",
    "\n",
    "# ensure that we have implemented our environment correctly\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define and train our model\n",
    "# todo: feel free to pick another model or policy, for more information see https://stable-baselines3.readthedocs.io/en/master/guide/algos.html\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe53dd",
   "metadata": {},
   "source": [
    "### To Do\n",
    "#### Carrot and sticks\n",
    "In the first block below we can implement the second rewarder\n",
    "\n",
    "2. Provides a positive reward when the goal is reached. Gives a negative reward (penalty) when the agent bumps into a wall. Otherwise, nothing.\n",
    "\n",
    "After defining the rewarder we can train an agent by running the second code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrotAndSticks(Rewarder):\n",
    "    def __init__(self):\n",
    "        # todo\n",
    "        # self._reward_range = (0, 0)\n",
    "\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        # todo: return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f66728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: test your environment like above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
