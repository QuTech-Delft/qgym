{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c91bb9",
   "metadata": {},
   "source": [
    "# Escaping the maze\n",
    "In this notebook we will cover the basics of a reinforcement learning (RL) environment.\n",
    "\n",
    "Specifically, we will cover the observation, action, and state space following the example of a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from qgym.environment import Environment\n",
    "from qgym.rewarder import Rewarder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613c7f6",
   "metadata": {},
   "source": [
    "## Map of the maze\n",
    "\n",
    "Our maze will have 4 different field types.\n",
    "\n",
    "- `S`: start position\n",
    "- `F`: a free field\n",
    "- `W`: a wall\n",
    "- `G`: the goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_map_4x4 = [\"FSFF\", \"SWFW\", \"FFFW\", \"WFFG\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9c75",
   "metadata": {},
   "source": [
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d25ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeRewarder(Rewarder):\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        row, col = new_state[\"position\"]\n",
    "        if new_state[\"maze_map\"][row][col] == b\"G\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Environment):\n",
    "    def __init__(self, maze_map):\n",
    "        maze_map = np.asarray(maze_map, dtype=\"c\")\n",
    "\n",
    "        self.nrows = maze_map.shape[0]\n",
    "        self.ncols = maze_map.shape[1]\n",
    "\n",
    "        self.start_position_distribution = (maze_map == b\"S\").ravel().astype(\"float64\")\n",
    "        self.start_position_distribution /= self.start_position_distribution.sum()\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)  # {0,1,2,3}\n",
    "        self.observation_space = gym.spaces.Discrete(self.nrows * self.ncols)\n",
    "        self._state = {\"position\": None, \"maze_map\": maze_map}\n",
    "        self._rewarder = MazeRewarder()\n",
    "\n",
    "    def rowcol_to_pos(self, row, col):\n",
    "        return row * self.nrows + col\n",
    "\n",
    "    def pos_to_rowcol(self, pos):\n",
    "        return int(pos / self.nrows), pos % self.nrows\n",
    "\n",
    "    def reset(self, *, seed=None, return_info=False):\n",
    "        start_position = self.rng.choice(\n",
    "            self.nrows * self.ncols, p=self.start_position_distribution\n",
    "        )\n",
    "        self._state[\"position\"] = self.pos_to_rowcol(start_position)\n",
    "\n",
    "        return super().reset(seed=seed, return_info=return_info)\n",
    "\n",
    "    def _update_state(self, action):\n",
    "        row, col = self._state[\"position\"]\n",
    "\n",
    "        # compute new position\n",
    "        if action == 0:  # up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 1:  # right\n",
    "            col = min(col + 1, self.ncols - 1)\n",
    "        elif action == 2:  # down\n",
    "            row = min(row + 1, self.nrows - 1)\n",
    "        elif action == 3:  # left\n",
    "            col = max(col - 1, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action supplied.\")\n",
    "\n",
    "        # go to new position if it is not a wall\n",
    "        if self._state[\"maze_map\"][row][col] != b\"W\":\n",
    "            self._state[\"position\"] = (row, col)\n",
    "        # else we stay where we are\n",
    "\n",
    "    def _obtain_observation(self):\n",
    "        return self.rowcol_to_pos(*self._state[\"position\"])\n",
    "\n",
    "    def _is_done(self):\n",
    "        row, col = self._state[\"position\"]\n",
    "        return self._state[\"maze_map\"][row][col] == b\"G\"\n",
    "\n",
    "    def _obtain_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _compute_reward(self, old_state, action):\n",
    "        return super()._compute_reward(\n",
    "            old_state=old_state, action=action, new_state=self._state\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: run script\n",
    "env = Maze(maze_map_4x4)\n",
    "check_env(env)\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f66728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
