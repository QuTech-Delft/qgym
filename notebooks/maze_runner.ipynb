{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c91bb9",
   "metadata": {},
   "source": [
    "# Maze runner\n",
    "In this notebook we will cover the basics of a reinforcement learning (RL) environment.\n",
    "\n",
    "Specifically, we will cover the observation, action, and state space following the example of a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from qgym.environment import Environment\n",
    "from qgym.rewarder import Rewarder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613c7f6",
   "metadata": {},
   "source": [
    "### Maze layout\n",
    "\n",
    "Our maze will have 4 different field types.\n",
    "\n",
    "- `F`: a free field\n",
    "- `W`: a wall\n",
    "- `G`: the goal\n",
    "- `S`: possible start position(s); also a free field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_map_4x4 = [\"FSFF\", \"SWFW\", \"FFFW\", \"WFFG\"]\n",
    "print(\"\\n\".join(maze_map_4x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9c75",
   "metadata": {},
   "source": [
    "### Environment spaces\n",
    "\n",
    "A Reinforcement Learning environment consists of several spaces that describe its workings:\n",
    "- State space\n",
    "- Action space\n",
    "- Observation space\n",
    "\n",
    "#### State space\n",
    "Current position and map of the maze\n",
    "\n",
    "#### Action space\n",
    "\n",
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT\n",
    "\n",
    "#### Observation space\n",
    "Current position\n",
    "\n",
    "_Hint: OpenAI Gym provides a ready-to-use [`Discrete`](https://www.gymlibrary.ml/content/spaces/#discrete) action/observation space._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e5708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Environment):\n",
    "    def __init__(self, maze_map):\n",
    "        maze_map = np.asarray(maze_map, dtype=\"c\")  # todo\n",
    "\n",
    "        self.nrows = maze_map.shape[0]\n",
    "        self.ncols = maze_map.shape[1]\n",
    "\n",
    "        self.start_position_distribution = (maze_map == b\"S\").ravel().astype(\"float64\")\n",
    "        self.start_position_distribution /= (\n",
    "            self.start_position_distribution.sum()\n",
    "        )  # todo\n",
    "\n",
    "        self.action_space = gym.spaces.Discrete(4)  # {0,1,2,3}\n",
    "        self.observation_space = gym.spaces.Discrete(self.nrows * self.ncols)\n",
    "        self._state = {\"position\": None, \"maze_map\": maze_map}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca486c97",
   "metadata": {},
   "source": [
    "### Initial position\n",
    "At the beginning of each episode, the environment should provide the agent with a fresh start, without any leftovers over the previous iteration.\n",
    "\n",
    "In this environment, the fresh start consists of a randomly selected initial position from all possible _start positions_.\n",
    "\n",
    "_Hint: Each environment has a random number generator `self.rng` with a [`choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) method._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Maze):\n",
    "    def rowcol_to_pos(self, row, col):\n",
    "        return row * self.nrows + col\n",
    "\n",
    "    def pos_to_rowcol(self, pos):\n",
    "        return int(pos / self.nrows), pos % self.nrows\n",
    "\n",
    "    def reset(self, *, seed=None, return_info=False):\n",
    "        start_position = self.rng.choice(\n",
    "            self.nrows * self.ncols, p=self.start_position_distribution\n",
    "        )\n",
    "        self._state[\"position\"] = self.pos_to_rowcol(start_position)\n",
    "\n",
    "        return super().reset(seed=seed, return_info=return_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaef3d",
   "metadata": {},
   "source": [
    "### First steps\n",
    "In order to let the agent actually get out of the maze it needs to be able to move to an adjacent field. Recall that we defined our action space as:\n",
    "\n",
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT.\n",
    "\n",
    "_Note: How should we deal with bumping into the wall?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Maze):\n",
    "    def _update_state(self, action):\n",
    "        row, col = self._state[\"position\"]\n",
    "\n",
    "        # compute new position\n",
    "        if action == 0:  # up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 1:  # right\n",
    "            col = min(col + 1, self.ncols - 1)\n",
    "        elif action == 2:  # down\n",
    "            row = min(row + 1, self.nrows - 1)\n",
    "        elif action == 3:  # left\n",
    "            col = max(col - 1, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action supplied.\")\n",
    "\n",
    "        # go to new position if it is not a wall\n",
    "        if self._state[\"maze_map\"][row][col] != b\"W\":\n",
    "            self._state[\"position\"] = (row, col)\n",
    "        # else we stay where we are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133121dc",
   "metadata": {},
   "source": [
    "### Observational awareness\n",
    "Our environment is nearly done, but we still need to provide our agent a set of 'eyes'.\n",
    "\n",
    "Specifically we need to inform the agent of 2 more things:\n",
    "\n",
    "- The current position (our observation).\n",
    "- Whether we have reached the exit (are we done?)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Maze):\n",
    "    def _obtain_observation(self):\n",
    "        return self.rowcol_to_pos(*self._state[\"position\"])\n",
    "\n",
    "    def _is_done(self):\n",
    "        row, col = self._state[\"position\"]\n",
    "        return self._state[\"maze_map\"][row][col] == b\"G\"\n",
    "\n",
    "    def _obtain_info(self):\n",
    "        return {}\n",
    "\n",
    "    def _compute_reward(self, old_state, action):\n",
    "        return super()._compute_reward(\n",
    "            old_state=old_state, action=action, new_state=self._state\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4008e",
   "metadata": {},
   "source": [
    "### Carrot and Stick\n",
    "The final step towards completing our `Maze` environment is about giving feedback. We can give 3 types of feedback:\n",
    "- positive feedback (carrot)\n",
    "- negative feedback (stick)\n",
    "- neutral feedback\n",
    "\n",
    "In Reinforcement Learning, feedback is given by means of rewards. The choice of rewarder function has a lot of influence on the learning ability of the agent.\n",
    "\n",
    "Only providing rewards might lead to slow training and too much exploration. However, big penalties could make the agent skip exploration and stick to a safe, potentially non-optimal path.\n",
    "\n",
    "Below is room for two rewarders `CarrotOnly` (1) and `CarrotAndSticks` (2):\n",
    "\n",
    "1. Provides a positive reward only when the goal is reached. Does nothing otherwise\n",
    "2. Provides a positive reward when the goal is reached. Gives a negative reward (penalty) when the agent bumps into a wall. Otherwise, nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrotOnly(Rewarder):\n",
    "    def __init__(self):\n",
    "        self._reward_range = (0, 1)\n",
    "\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        row, col = new_state[\"position\"]\n",
    "        if new_state[\"maze_map\"][row][col] == b\"G\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bcdd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrotAndSticks(Rewarder):\n",
    "    def __init__(self):\n",
    "        self._reward_range = (-1, 10)\n",
    "\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        row, col = new_state[\"position\"]\n",
    "\n",
    "        if new_state[\"position\"] == old_state[\"position\"]:\n",
    "            return -1\n",
    "        elif new_state[\"maze_map\"][row][col] == b\"G\":\n",
    "            return 10\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1789c",
   "metadata": {},
   "source": [
    "### Training an agent\n",
    "\n",
    "Below is room to train our environment using either of the two rewarders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment and rewarder\n",
    "env = Maze(maze_map_4x4)\n",
    "env.rewarder = CarrotOnly()\n",
    "\n",
    "# ensure that we have implemented our environment correctly\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define and train our model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f66728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment and rewarder\n",
    "env = Maze(maze_map_4x4)\n",
    "env.rewarder = CarrotAndSticks()\n",
    "\n",
    "# ensure that we have implemented our environment correctly\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define and train our model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(int(1e5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
