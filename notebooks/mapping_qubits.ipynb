{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c032998e",
   "metadata": {},
   "source": [
    "# Mapping qubits\n",
    "In this notebook we will cover the QGym `InitialMapping` environment.\n",
    "\n",
    "This environment is aimed at solving the problem of mapping virtual to physical qubits that have a certain topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4505d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.generators import gnp_random_graph\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from qgym.envs.initial_mapping import InitialMapping\n",
    "from qgym.envs.initial_mapping.initial_mapping_rewarders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rgb(step, rgb_array):\n",
    "    \"\"\"\n",
    "    Convenience method that we will use later on to display our results.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.title(f\"Step {step}\", fontsize=40)\n",
    "    plt.imshow(rgb_array)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4328e243",
   "metadata": {},
   "source": [
    "### Connection and interaction graph\n",
    "\n",
    "The initial mapping problem is focussed around two graphs:\n",
    "\n",
    "- connection graph: hardware layout describing the connections between physical qubits\n",
    "- interaction graph: software layout describing which virtual qubits interact in the particular quantum program\n",
    "\n",
    "The goal of the initial mapping problem is to find an optimal one-to-one between the virtual qubits of the interaction graph and the physical qubits of the connection graph.\n",
    "\n",
    "For now, we will consider an optimal mapping to be any mapping where the number of edges of the mapped interaction graph that do not coincide with edges of the connection graph is minimal.\n",
    "\n",
    "#### Toy hardware\n",
    "\n",
    "To explain this concept in more detail we start by defining a toy connection graph and by taking a look at some potential interaction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(0, 2)\n",
    "connection_graph.add_edge(0, 3)\n",
    "nx.draw(connection_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d15ce2b",
   "metadata": {},
   "source": [
    "Now let's take a look at some random interaction graphs, and think about how these can be best mapped on the connection graph.\n",
    "\n",
    "_We can simply generate random graphs using [`gnp_random_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.gnp_random_graph.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_interaction_graph(connection_graph):\n",
    "    p = np.random.rand()  # edge probability\n",
    "    n = connection_graph.number_of_nodes()\n",
    "    return gnp_random_graph(n, p)\n",
    "    \n",
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a1748",
   "metadata": {},
   "source": [
    "### `InitialMapping` environment\n",
    "\n",
    "todo: describe this environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e2b39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: show the state after a reset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39358b25",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Human Intelligence\n",
    "\n",
    "todo: describe environment specifics\n",
    "\n",
    "Since this environment is still quite straightforward, we should be able to solve this case optimally by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51956c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c127bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((0,0))\n",
    "render_rgb(1, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29782ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((1,1))\n",
    "render_rgb(2, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f29e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((2,2))\n",
    "render_rgb(3, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adffa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((3,3))\n",
    "render_rgb(4, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fb16c0",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "Let's check if our reinforcement learning agent is capable of solving this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56766025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_1\")\n",
    "obs = env.reset(interaction_graph=connection_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40047cf6",
   "metadata": {},
   "source": [
    "Let's try another interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.remove_edge(1, 3)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_1\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff004cb3",
   "metadata": {},
   "source": [
    "Just to be sure, one more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.add_edge(3, 2)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82b8dff",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### More realistic hardware\n",
    "\n",
    "Having seen that we are able to train an agent on a toy environment, let's take a look at a more realistic hardware topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(1, 2)\n",
    "connection_graph.add_edge(2, 0)\n",
    "connection_graph.add_edge(2, 3)\n",
    "connection_graph.add_edge(3, 4)\n",
    "connection_graph.add_edge(4, 2)\n",
    "nx.draw(connection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e6))\n",
    "model.save(\"initial_mapping_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1caca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_2\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad23ff7e",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Connection fidelity\n",
    "\n",
    "Up to now, we have only used the `EpisodeRewarder`. However, we can also define rewarders differently in order to improve performance.\n",
    "\n",
    "Alternatively, we can define a different rewarder in order to change the objective. Since, for most quantum hardware not every connection has the same fidelity we might want to take this into account for the computation of our reward, such that our agent attempts to find a solution which not only requires a small amount of swap gates but also takes fidelity into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1, weight=1)\n",
    "connection_graph.add_edge(1, 2, weight=1)\n",
    "connection_graph.add_edge(2, 0, weight=1)\n",
    "connection_graph.add_edge(2, 3, weight=0.5)\n",
    "connection_graph.add_edge(3, 4, weight=0.5)\n",
    "connection_graph.add_edge(4, 2, weight=0.5)\n",
    "\n",
    "# display graph with edge weights\n",
    "pos=nx.spring_layout(connection_graph, seed=0)\n",
    "edge_labels = nx.get_edge_attributes(connection_graph, \"weight\")\n",
    "nx.draw(connection_graph, pos, with_labels=True)\n",
    "nx.draw_networkx_edge_labels(connection_graph, pos, edge_labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9e6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FidelityEpisodeRewarder(BasicRewarder):\n",
    "    \n",
    "    def compute_reward(\n",
    "        self,\n",
    "        *,\n",
    "        old_state: Dict[Any, Any],\n",
    "        action: NDArray[np.int_],\n",
    "        new_state: Dict[Any, Any],\n",
    "    ):\n",
    "        if self._is_illegal(action, old_state):\n",
    "            return self._illegal_action_penalty\n",
    "\n",
    "        if (\n",
    "            len(new_state[\"physical_qubits_mapped\"])\n",
    "            != new_state[\"connection_graph_matrix\"].shape[0]\n",
    "        ):\n",
    "            return 0\n",
    "\n",
    "        return self._compute_state_reward(new_state)\n",
    "\n",
    "\n",
    "    def _compute_state_reward(self, state: Dict[Any, Any]) -> float:\n",
    "        reward = 0.0\n",
    "        for interaction_i, interaction_j in zip(*state[\"interaction_graph_matrix\"].nonzero()):\n",
    "            mapped_interaction_i = state[\"mapping_dict\"][interaction_i]\n",
    "            mapped_interaction_j = state[\"mapping_dict\"][interaction_j]\n",
    "            \n",
    "            edge_fidelity = state[\"interaction_graph_matrix\"][interaction_i, interaction_j]\n",
    "            if edge_fidelity == 0:\n",
    "                reward += self._penalty_per_edge\n",
    "            else:\n",
    "                reward += edge_fidelity * self._reward_per_edge\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79126199",
   "metadata": {},
   "source": [
    "Does this rewarder do the job? Or, is some more tweaking required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc92c738",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = FidelityEpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e6))\n",
    "model.save(\"initial_mapping_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee21be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb1b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_3\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
