{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c032998e",
   "metadata": {},
   "source": [
    "# Mapping qubits\n",
    "In this notebook we will cover the QGym `InitialMapping` environment.\n",
    "\n",
    "This environment is aimed at solving the problem of mapping virtual to physical qubits that have a certain topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4505d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.generators import gnp_random_graph\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from qgym.envs.initial_mapping import InitialMapping\n",
    "from qgym.envs.initial_mapping.initial_mapping_rewarders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rgb(step, rgb_array):\n",
    "    \"\"\"\n",
    "    Convenience method that we will use later on to display our results.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.title(f\"Step {step}\", fontsize=40)\n",
    "    plt.imshow(rgb_array)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233547f7",
   "metadata": {},
   "source": [
    "### Connection and interaction graph\n",
    "\n",
    "The initial mapping problem is focussed around two graphs:\n",
    "\n",
    "- connection graph: hardware layout describing the connections between physical qubits\n",
    "- interaction graph: software layout describing which virtual qubits interact in the particular quantum program\n",
    "\n",
    "The goal of the initial mapping problem is to find an optimal one-to-one between the virtual qubits of the interaction graph and the physical qubits of the connection graph.\n",
    "\n",
    "For now, we will consider an optimal mapping to be any mapping where the number of edges of the mapped interaction graph that do not coincide with edges of the connection graph is minimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16eed55",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "\n",
    "### Toy hardware\n",
    "\n",
    "To explain this concept in more detail we start by defining a toy connection graph and by taking a look at some potential interaction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(0, 2)\n",
    "connection_graph.add_edge(0, 3)\n",
    "nx.draw(connection_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d38efb1",
   "metadata": {},
   "source": [
    "Now let's take a look at some random interaction graphs, and think about how these can be best mapped on the connection graph.\n",
    "\n",
    "_We can simply generate random graphs using [`gnp_random_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.gnp_random_graph.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4739e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_interaction_graph(connection_graph):\n",
    "    p = np.random.rand()  # edge probability\n",
    "    n = connection_graph.number_of_nodes()\n",
    "    return gnp_random_graph(n, p)\n",
    "    \n",
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b92a1",
   "metadata": {},
   "source": [
    "### Human Intelligence\n",
    "\n",
    "todo: describe environment specifics\n",
    "\n",
    "Since this environment is still quite straightforward, we should be able to solve this case optimally by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f33001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa8945",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((0,0))\n",
    "render_rgb(1, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abaa709",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((1,1))\n",
    "render_rgb(2, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4084bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((2,2))\n",
    "render_rgb(3, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a94ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((3,3))\n",
    "render_rgb(4, env.render(mode=\"rgb_array\"))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f79c41",
   "metadata": {},
   "source": [
    "### Reinforcement learning\n",
    "\n",
    "Let's check if our reinforcement learning agent is capable of solving this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56766025",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(interaction_graph=connection_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcc31d",
   "metadata": {},
   "source": [
    "Let's try another interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.remove_edge(1, 3)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd2e2f9",
   "metadata": {},
   "source": [
    "Just to be sure, one more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.add_edge(3, 2)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba146556",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### More realistic hardware\n",
    "\n",
    "Having seen that we are able to train an agent on a toy environment, let's take a look at a more realistic hardware topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(1, 2)\n",
    "connection_graph.add_edge(2, 0)\n",
    "connection_graph.add_edge(2, 3)\n",
    "connection_graph.add_edge(3, 4)\n",
    "connection_graph.add_edge(4, 2)\n",
    "connection_graph.number_of_nodes()\n",
    "nx.draw(connection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e6))\n",
    "model.save(\"saved_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = fast_gnp_random_graph(5, 0.5)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1caca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "model = PPO.load(\"saved_model\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7cdf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the same with the other two rewarders that we have defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QAP rewarder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
