{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c032998e",
   "metadata": {},
   "source": [
    "# Mapping qubits\n",
    "In this notebook we will cover the QGym `InitialMapping` environment.\n",
    "\n",
    "This environment is aimed at solving the problem of mapping virtual to physical qubits that have a certain topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4505d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from networkx.generators import gnp_random_graph\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from qgym.envs.initial_mapping import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rgb(env):\n",
    "    \"\"\"Convenience method that we will use later on to display our results.\"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.title(f\"Step {env._state.steps_done}\", fontsize=40)\n",
    "    plt.imshow(env.render())\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda37cd9",
   "metadata": {},
   "source": [
    "### Connection and interaction graph\n",
    "\n",
    "The initial mapping problem is focussed around two graphs:\n",
    "\n",
    "- connection graph: hardware layout describing the connections between physical qubits\n",
    "- interaction graph: software layout describing which virtual qubits interact in the particular quantum program\n",
    "\n",
    "The goal of the initial mapping problem is to find an optimal one-to-one between the virtual qubits of the interaction graph and the physical qubits of the connection graph.\n",
    "\n",
    "For now, we will consider an optimal mapping to be any mapping where the number of edges of the mapped interaction graph that do not coincide with edges of the connection graph is minimal.\n",
    "\n",
    "#### Toy hardware\n",
    "\n",
    "To explain this concept in more detail we start by defining a toy connection graph and by taking a look at some potential interaction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edges_from([(0, 1), (0, 2), (0, 3)])\n",
    "nx.draw(connection_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1de923",
   "metadata": {},
   "source": [
    "Now let's take a look at some random interaction graphs, and think about how these can be best mapped on the connection graph.\n",
    "\n",
    "### To Do\n",
    "Implement the `generate_random_interaction_graph` in the code block below.\n",
    "\n",
    "_We can simply generate random graphs using [`gnp_random_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.gnp_random_graph.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_interaction_graph(connection_graph):\n",
    "    rng = np.random.default_rng()\n",
    "    p = rng.rand()  # edge probability\n",
    "    n = connection_graph.number_of_nodes()\n",
    "    return gnp_random_graph(n, p)\n",
    "\n",
    "\n",
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff55e3",
   "metadata": {},
   "source": [
    "### `InitialMapping` environment\n",
    "\n",
    "The most simple `InitialMapping` environment can be initialized by providing just a connection graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe270a85",
   "metadata": {},
   "source": [
    "#### State space\n",
    "The state space is described by a `State` with the following structure:\n",
    "\n",
    "- `steps_done`: Number of steps done since the last reset.\n",
    "- `num_nodes`: Number of *physical* qubits.\n",
    "- `graphs`: Dictionary containing the the interaction graph, connection graph and a interaction graph generator.\n",
    "- `mapping`: Array of which the index represents a physical qubit, and the value a virtual qubit. A value of ``num_nodes + 1`` represents the case when nothing is mapped to the physical qubit yet. (Used for observations)\n",
    "- `mapping_dict`: Dictionary that maps logical qubits (keys) to physical qubit (values).\n",
    "- `mapped_qubits`: Dictionary with a two Sets containing all mapped physical and logical qubits.\n",
    "\n",
    "### To Do\n",
    "Take a look at the state space in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env._state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e0931",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "A valid action is a tuple of integers $(i,j)$, such that $0 \\leq i,j < n$ where $n$ is the number of physical qubits. The action $(i,j)$ maps virtual qubit $j$ to phyiscal qubit $i$ when this action is legal. An action is legal when:\n",
    "1. virtual qubit $i$ has not been mapped to another physical qubit; and\n",
    "2. no other virual qubit has been mapped to physical qubit $j$.\n",
    "\n",
    "### To Do\n",
    "Take a look at the action space in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72160a43",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "Each element in the observation space is a dictionary with 2 entries:\n",
    "- `mapping`: the current state of the mapping\n",
    "- `interaction_matrix`: the flattened adjacency matrix of the interaction graph\n",
    "\n",
    "### To Do\n",
    "Take a look at the observation space and see what the reset() method returns in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b441f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print()\n",
    "obs, extra_info = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ae6a1",
   "metadata": {},
   "source": [
    "#### Rewarders\n",
    "\n",
    "We have pre-defined 3 different rewarders, all of which return a penalty when an illegal action is attempted, for a valid action their behaviour is as follows:\n",
    "\n",
    "- `BasicRewarder`: Reward is computed over all edges that have been mapped so far.\n",
    "- `SingleStepRewader`: Reward is computed over all new edges that have been mapped due to this action.\n",
    "- `EpisodeRewarder`: Only the final step results in a reward.\n",
    "\n",
    "All these rewarders can be tweaked by altering either of their parameters:\n",
    "\n",
    "- `illegal_action_penalty`: penalty given for attempting an illegal action (should be non-positive)\n",
    "- `reward_per_edge`: reward giving for correctly mapped edges (should be non-negative)\n",
    "- `penalty_per_edge`: penalty given for incorrectly mapped edges (should be non-positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f2f27",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Human Intelligence\n",
    "Let's attempt to determine the optimal mapping from our randomly generated interaction graph to our toy connection graph. Since this environment is still quite straightforward, we should be able to solve this case optimally by hand.\n",
    "\n",
    "Don't forget to take a look at the obtained rewards and observations after each step.\n",
    "\n",
    "_Note: It might be that multiple mappings our optimal._\n",
    "\n",
    "### To Do\n",
    "Pick your favorite rewarder in the first code block below.\n",
    "Next, try to solve the problem by giving the actions which you think are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph, render_mode=\"rgb_array\")\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "obs, extra_info = env.reset(options={\"interaction_graph\": interaction_graph})\n",
    "print(f\"observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, truncated, info = env.step((0, 0))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, truncated, info = env.step((1, 1))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f421af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, truncated, info = env.step((2, 2))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cfb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, truncated, info = env.step((3, 3))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc50d2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "Can we achieve the same using reinforcement learning?\n",
    "\n",
    "Does changing the rewarder and/or its parameter give better results?\n",
    "\n",
    "### To Do\n",
    "Train a model on this environment in the first code block below.\n",
    "Next, run the second block to see how well the agent performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56766025",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph, render_mode=\"rgb_array\")\n",
    "model = PPO.load(\"initial_mapping_1\")\n",
    "\n",
    "obs, extra_info = env.reset(options={\"interaction_graph\": connection_graph})\n",
    "for _ in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, truncated, info = env.step(action)\n",
    "    render_rgb(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198cf7c",
   "metadata": {},
   "source": [
    "Let's try another interaction graph.\n",
    "\n",
    "### To Do\n",
    "Design a nice interaction graph in the first code block.\n",
    "Next, run the second block to see if the agent can map it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.remove_edge(0, 2)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_1\")\n",
    "obs, extra_info = env.reset(options={\"interaction_graph\": interaction_graph})\n",
    "for _ in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, trucated, info = env.step(action)\n",
    "    render_rgb(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1a18b",
   "metadata": {},
   "source": [
    "Just to be sure, one more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.add_edge(3, 2)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, extra_info = env.reset(options={\"interaction_graph\": interaction_graph})\n",
    "for _ in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, truncated, info = env.step(action)\n",
    "    render_rgb(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13d19f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### More realistic hardware\n",
    "\n",
    "Having seen that we are able to train an agent on a toy environment, let's take a look at a more realistic hardware topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edges_from([(0, 1), (1, 2), (2, 0), (2, 3), (3, 4), (4, 2)])\n",
    "nx.draw(connection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36b258c",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Use the first code block to create a new environment with the new connection graph, set a rewarder and train an agent.\n",
    "Use the second code block to design a interaction graph of your choice.\n",
    "Finally, use the third code block to see how well the agent performs on your interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1caca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph, render_mode=\"rgb_array\")\n",
    "model = PPO.load(\"initial_mapping_2\")\n",
    "\n",
    "obs, extra_info = env.reset(options={\"interaction_graph\": interaction_graph})\n",
    "for _ in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, truncated, info = env.step(action)\n",
    "    render_rgb(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9c32a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Connection fidelity\n",
    "\n",
    "Up to now, we have consider interaction graphs without fidelity. However, we can also train agents to learn how to deal with fidelity.\n",
    "\n",
    "Most digital quantum computers do not have the same fidelity on every edge. Hence, we might want to take this into account for the computation of our reward. Meaning, that the agent would attempt to find a solution which not only requires a small amount of swap gates but also takes edge fidelities into account.\n",
    "\n",
    "### To Do\n",
    "Define a weighted connection graph in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1, weight=1)\n",
    "connection_graph.add_edge(1, 2, weight=1)\n",
    "connection_graph.add_edge(2, 0, weight=1)\n",
    "connection_graph.add_edge(2, 3, weight=0.5)\n",
    "connection_graph.add_edge(3, 4, weight=0.5)\n",
    "connection_graph.add_edge(4, 2, weight=0.5)\n",
    "\n",
    "# display graph with edge weights\n",
    "pos = nx.spring_layout(connection_graph, seed=0)\n",
    "edge_labels = nx.get_edge_attributes(connection_graph, \"weight\")\n",
    "nx.draw(connection_graph, pos, with_labels=True)\n",
    "nx.draw_networkx_edge_labels(connection_graph, pos, edge_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50865285",
   "metadata": {},
   "source": [
    "Time for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963abbf",
   "metadata": {},
   "source": [
    "How does fidelity influence your training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(connection_graph=connection_graph, render_mode=\"rgb_array\")\n",
    "model = PPO.load(\"initial_mapping_3\")\n",
    "\n",
    "obs, extra_info = env.reset(options={\"interaction_graph\": interaction_graph})\n",
    "for _ in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, truncated, info = env.step(action)\n",
    "    render_rgb(env)\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
