{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c032998e",
   "metadata": {},
   "source": [
    "# Mapping qubits\n",
    "In this notebook we will cover the QGym `InitialMapping` environment.\n",
    "\n",
    "This environment is aimed at solving the problem of mapping virtual to physical qubits that have a certain topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4505d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.generators import gnp_random_graph\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from qgym.envs.initial_mapping import InitialMapping\n",
    "from qgym.envs.initial_mapping.initial_mapping_rewarders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rgb(step, rgb_array):\n",
    "    \"\"\"\n",
    "    Convenience method that we will use later on to display our results.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.title(f\"Step {step}\", fontsize=40)\n",
    "    plt.imshow(rgb_array)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda37cd9",
   "metadata": {},
   "source": [
    "### Connection and interaction graph\n",
    "\n",
    "The initial mapping problem is focussed around two graphs:\n",
    "\n",
    "- connection graph: hardware layout describing the connections between physical qubits\n",
    "- interaction graph: software layout describing which virtual qubits interact in the particular quantum program\n",
    "\n",
    "The goal of the initial mapping problem is to find an optimal one-to-one between the virtual qubits of the interaction graph and the physical qubits of the connection graph.\n",
    "\n",
    "For now, we will consider an optimal mapping to be any mapping where the number of edges of the mapped interaction graph that do not coincide with edges of the connection graph is minimal.\n",
    "\n",
    "#### Toy hardware\n",
    "\n",
    "To explain this concept in more detail we start by defining a toy connection graph and by taking a look at some potential interaction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(0, 2)\n",
    "connection_graph.add_edge(0, 3)\n",
    "nx.draw(connection_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1de923",
   "metadata": {},
   "source": [
    "Now let's take a look at some random interaction graphs, and think about how these can be best mapped on the connection graph.\n",
    "\n",
    "_We can simply generate random graphs using [`gnp_random_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.gnp_random_graph.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_interaction_graph(connection_graph):\n",
    "    p = np.random.rand()  # edge probability\n",
    "    n = connection_graph.number_of_nodes()\n",
    "    return gnp_random_graph(n, p)\n",
    "\n",
    "\n",
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff55e3",
   "metadata": {},
   "source": [
    "### `InitialMapping` environment\n",
    "\n",
    "The `InitialMapping` environment can be initialized by providing:\n",
    "- a connection graph; and\n",
    "- an edge probability (i.e. the probability that an edge in the interaction graph exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe270a85",
   "metadata": {},
   "source": [
    "#### State space\n",
    "The state space is described by a dictionary with the following structure:\n",
    "\n",
    "- `connection_graph_matrix`: A sparse adjacency matrix of the connection graph.\n",
    "- `num_nodes`: Number of physical qubits\n",
    "- `interaction_graph_matrix`: A sparse adjacency matrix of the interaction graph. _(Should have the same number of nodes as the connection graph.)_\n",
    "- `steps_done`: Number of steps done since the last reset.\n",
    "- `mapping`: A list of which the index represents a physical qubit and the value a virtual qubit (is set to `num_nodes` + 1 when nothing is mapped to the physical qubit yet).\n",
    "- `mapping_dict`: A dictionary that maps logical qubits to physical qubit.\n",
    "- `physical_qubits_mapped`. A set containing all physical qubits that have been mapped already.\n",
    "- `virtual_qubits_mapped`. A set containing all virtual qubits that have been mapped already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "print(env._state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e0931",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "A valid action is a tuple of integers $(i,j)$, such that $0 \\leq i,j < n$ where $n$ is the number of physical qubits. The action $(i,j)$ maps virtual qubit $j$ to phyiscal qubit $i$ when this action is legal. An action is legal when:\n",
    "1. virtual qubit $i$ has not been mapped to another physical qubit; and\n",
    "2. no other virual qubit has been mapped to physical qubit $j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72160a43",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "Each element in the observation space is a dictionary with 2 entries:\n",
    "- `mapping`: the current state of the mapping\n",
    "- `interaction_matrix`: the flattened adjacency matrix of the interaction graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b441f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print()\n",
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ae6a1",
   "metadata": {},
   "source": [
    "#### Rewarders\n",
    "\n",
    "We have pre-defined 3 different rewarders, all of which return a penalty when an illegal action is attempted, for a valid action their behaviour is as follows:\n",
    "\n",
    "- `BasicRewarder`: Reward is computed over all edges that have been mapped so far.\n",
    "- `SingleStepRewader`: Reward is computed over all new edges that have been mapped due to this action.\n",
    "- `EpisodeRewarder`: Only the final step results in a reward.\n",
    "\n",
    "All these rewarders can be tweaked by altering either of their parameters:\n",
    "\n",
    "- `illegal_action_penalty`: penalty given for attempting an illegal action (should be non-positive)\n",
    "- `reward_per_edge`: reward giving for correctly mapped edges (should be non-negative)\n",
    "- `penalty_per_edge`: penalty given for incorrectly mapped edges (should be non-positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f2f27",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Human Intelligence\n",
    "Let's attempt to determine the optimal mapping from our randomly generated interaction graph to our toy connection graph. Since this environment is still quite straightforward, we should be able to solve this case optimally by hand.\n",
    "\n",
    "Don't forget to take a look at the obtained rewards and observations after each step.\n",
    "\n",
    "_Note: It might be that multiple mappings our optimal._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "print(f\"observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((0, 0))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(1, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((1, 1))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(2, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f421af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((2, 2))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(3, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cfb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, rewards, done, info = env.step((3, 3))\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(4, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc50d2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "Can we achieve the same using reinforcement learning?\n",
    "\n",
    "Does changing the rewarder and/or its parameter give better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=0)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56766025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_1\")\n",
    "obs = env.reset(interaction_graph=connection_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198cf7c",
   "metadata": {},
   "source": [
    "Let's try another interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.remove_edge(0, 2)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_1\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1a18b",
   "metadata": {},
   "source": [
    "Just to be sure, one more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = connection_graph.copy()\n",
    "interaction_graph.add_edge(3, 2)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13d19f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### More realistic hardware\n",
    "\n",
    "Having seen that we are able to train an agent on a toy environment, let's take a look at a more realistic hardware topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(1, 2)\n",
    "connection_graph.add_edge(2, 0)\n",
    "connection_graph.add_edge(2, 3)\n",
    "connection_graph.add_edge(3, 4)\n",
    "connection_graph.add_edge(4, 2)\n",
    "nx.draw(connection_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = EpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1caca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_2\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9c32a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Connection fidelity\n",
    "\n",
    "Up to now, we have only used the `EpisodeRewarder`. However, we can also define rewarders differently in order to improve performance.\n",
    "\n",
    "Alternatively, we can define a different rewarder in order to change the objective. Since, for most quantum hardware not every connection has the same fidelity we might want to take this into account for the computation of our reward, such that our agent attempts to find a solution which not only requires a small amount of swap gates but also takes fidelity into account.\n",
    "\n",
    "Let's first define a weighted connection graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1, weight=1)\n",
    "connection_graph.add_edge(1, 2, weight=1)\n",
    "connection_graph.add_edge(2, 0, weight=1)\n",
    "connection_graph.add_edge(2, 3, weight=0.5)\n",
    "connection_graph.add_edge(3, 4, weight=0.5)\n",
    "connection_graph.add_edge(4, 2, weight=0.5)\n",
    "\n",
    "# display graph with edge weights\n",
    "pos = nx.spring_layout(connection_graph, seed=0)\n",
    "edge_labels = nx.get_edge_attributes(connection_graph, \"weight\")\n",
    "nx.draw(connection_graph, pos, with_labels=True)\n",
    "nx.draw_networkx_edge_labels(connection_graph, pos, edge_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1de0f",
   "metadata": {},
   "source": [
    "And now we can implement a new rewarder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FidelityEpisodeRewarder(BasicRewarder):\n",
    "    def compute_reward(\n",
    "        self,\n",
    "        *,\n",
    "        old_state: Dict[Any, Any],\n",
    "        action: NDArray[np.int_],\n",
    "        new_state: Dict[Any, Any],\n",
    "    ):\n",
    "        if self._is_illegal(action, old_state):\n",
    "            return self._illegal_action_penalty\n",
    "\n",
    "        if (\n",
    "            len(new_state[\"physical_qubits_mapped\"])\n",
    "            != new_state[\"connection_graph_matrix\"].shape[0]\n",
    "        ):\n",
    "            return 0\n",
    "\n",
    "        return self._compute_state_reward(new_state)\n",
    "\n",
    "    def _compute_state_reward(self, state: Dict[Any, Any]) -> float:\n",
    "        reward = 0.0\n",
    "        for interaction_i, interaction_j in zip(\n",
    "            *state[\"interaction_graph_matrix\"].nonzero()\n",
    "        ):\n",
    "            mapped_interaction_i = state[\"mapping_dict\"][interaction_i]\n",
    "            mapped_interaction_j = state[\"mapping_dict\"][interaction_j]\n",
    "\n",
    "            edge_fidelity = state[\"interaction_graph_matrix\"][\n",
    "                interaction_i, interaction_j\n",
    "            ]\n",
    "            if edge_fidelity == 0:\n",
    "                reward += self._penalty_per_edge\n",
    "            else:\n",
    "                reward += edge_fidelity * self._reward_per_edge\n",
    "\n",
    "        return reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50865285",
   "metadata": {},
   "source": [
    "Time for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "env.rewarder = FidelityEpisodeRewarder(illegal_action_penalty=-10)\n",
    "check_env(env, warn=True)\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "\n",
    "model.learn(int(1e5))\n",
    "model.save(\"initial_mapping_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963abbf",
   "metadata": {},
   "source": [
    "Does this rewarder do the job? Or, is some more tweaking required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_3\")\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96b72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
