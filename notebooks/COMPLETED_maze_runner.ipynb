{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15c91bb9",
   "metadata": {},
   "source": [
    "# Maze runner\n",
    "In this notebook we will cover the basics of a reinforcement learning (RL) environment.\n",
    "\n",
    "Specifically, we will cover the observation, action, and state space following the example of a maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "from qgym.spaces import Discrete\n",
    "from qgym.templates import Environment, Rewarder, State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f613c7f6",
   "metadata": {},
   "source": [
    "### Maze layout\n",
    "\n",
    "In this notebook we will design our own maze, which the agent will learn to navigate through.\n",
    "This maze will have 4 different field types.\n",
    "\n",
    "- `F`: a free field\n",
    "- `W`: a wall\n",
    "- `G`: the goal\n",
    "- `S`: possible start position(s); also a free field\n",
    "\n",
    "### To Do\n",
    "Design a fun 4 by 4 maze by making a list of 4 letter strings. For example `[\"FSFF\", \"SWFW\", \"FFFW\", \"WFFG\"]` would be a fine maze.\n",
    "Note that we allow to have multiple starting points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_map_4x4 = [\"FSFF\", \"SWFW\", \"FFFW\", \"WFFG\"]\n",
    "print(\"\\n\".join(maze_map_4x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a9c75",
   "metadata": {},
   "source": [
    "### Environment spaces\n",
    "\n",
    "A Reinforcement Learning environment consists of several spaces that describe its workings:\n",
    "- State space\n",
    "- Action space\n",
    "- Observation space\n",
    "\n",
    "#### State space\n",
    "The state space contains the complete information of the current state of the environment.\n",
    "The state space is for internal use and is not used by the agent.\n",
    "\n",
    "In our maze environment the state consists of five items:\n",
    "- `position`: the current position of the agent in the maze\n",
    "- `maze_map`: a map of the maze\n",
    "- `start_position_distribution`: Distribution from which position an episode can start.\n",
    "- `nrows`: Number of rows in the maze grid.\n",
    "- `ncols`: Number of collumns in the maze grid.\n",
    "\n",
    "#### Action space\n",
    "The action space describes all potential actions that an agent can take in this environment.\n",
    "\n",
    "In our maze environment there are only 4 such actions:\n",
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT\n",
    "\n",
    "#### Observation space\n",
    "The observation space describes all potential observations that an agent can obtain from the environment. It is generally a subset (or a transformation thereof) of the state space.\n",
    "\n",
    "In our case the observation space contains all possible `position`s the agent can be at.\n",
    "\n",
    "#### Constraints on the spaces\n",
    "Reinforcement Learning agents often accept a limited amount of data types.\n",
    "The most commonly supported data types are `int`, `float`, `char` and arrays of these types.\n",
    "Therefore, we will transform the fun maze above into such an array.\n",
    "Furthermore, when setting up the environment, we will need a starting position.\n",
    "Since we allow for multiple starting positions, we should determine a probability distribution to say where we start in an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae136aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can transfrom our map to a 4x4 array of chars as follows:\n",
    "transformed_maze = np.asarray(maze_map_4x4, dtype=\"c\")\n",
    "print(f\"Maze:\\n{transformed_maze}\\n\")\n",
    "\n",
    "# we can compute a probability distribution over the start positions as follows\n",
    "distribution = (transformed_maze == b\"S\").ravel().astype(\"float64\")\n",
    "distribution /= distribution.sum()\n",
    "print(f\"Distribution:\\n{distribution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ee4333",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Setup state space by setting up the attributes `position`, `maze_map`, `start_position_distribution`, `nrows` and `ncols`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e619ca62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(State):\n",
    "    def __init__(self, maze_map):\n",
    "        maze_map = np.asarray(maze_map, dtype=\"c\")\n",
    "\n",
    "        self.start_position_distribution = (maze_map == b\"S\").ravel().astype(float)\n",
    "        self.start_position_distribution /= self.start_position_distribution.sum()\n",
    "\n",
    "        self.nrows = maze_map.shape[0]\n",
    "        self.ncols = maze_map.shape[1]\n",
    "        self.position = None\n",
    "        self.maze_map = maze_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca486c97",
   "metadata": {},
   "source": [
    "### Initial position\n",
    "At the beginning of each episode, the environment should provide the agent with a fresh start, without any leftovers over the previous iteration.\n",
    "\n",
    "In this environment, the fresh start consists of a randomly selected initial position from all possible _start positions_.\n",
    "\n",
    "### To Do\n",
    "First implement two utility methods `rowcol_to_pos` and `pos_to_rowcol`.\n",
    "Subsequently, implement the `reset` method (for which you will need the `pos_to_rowcol` method).\n",
    "\n",
    "_Hint: Each environment has a random number generator `self.rng` with a [`choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.Generator.choice.html) method._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(MazeState):\n",
    "    def rowcol_to_pos(self, row, col):\n",
    "        return row * self.nrows + col\n",
    "\n",
    "    def pos_to_rowcol(self, pos):\n",
    "        return int(pos / self.nrows), pos % self.nrows\n",
    "\n",
    "    def reset(self, *, seed=None, return_info=False):\n",
    "        start_position = self.rng.choice(\n",
    "            self.nrows * self.ncols, p=self.start_position_distribution\n",
    "        )\n",
    "        self.position = self.pos_to_rowcol(start_position)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49aaef3d",
   "metadata": {},
   "source": [
    "### First steps\n",
    "In order to let the agent actually get out of the maze it needs to be able to move to an adjacent field. Recall that we defined our action space as:\n",
    "\n",
    "- `0`: UP\n",
    "- `1`: RIGHT\n",
    "- `2`: DOWN\n",
    "- `3`: LEFT.\n",
    "\n",
    "### To Do\n",
    "Write the `update_state` method, which should update the state according to the given action.\n",
    "\n",
    "_Note: How should we deal with illegal actions (i.e. falling of the grid, bumping into a wall)?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(MazeState):\n",
    "    def update_state(self, action):\n",
    "        row, col = self.position\n",
    "\n",
    "        # compute new position\n",
    "        if action == 0:  # up\n",
    "            row = max(row - 1, 0)\n",
    "        elif action == 1:  # right\n",
    "            col = min(col + 1, self.ncols - 1)\n",
    "        elif action == 2:  # down\n",
    "            row = min(row + 1, self.nrows - 1)\n",
    "        elif action == 3:  # left\n",
    "            col = max(col - 1, 0)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action supplied.\")\n",
    "\n",
    "        # go to new position if it is not a wall\n",
    "        if self.maze_map[row][col] != b\"W\":\n",
    "            self.position = (row, col)\n",
    "        # else we stay where we are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133121dc",
   "metadata": {},
   "source": [
    "### Observational awareness\n",
    "Our environment is nearly done, with the `update_state` method the agent can move through the maze.\n",
    "However, remember that the agent can't see the state space!\n",
    "We still need to provide our agent a set of 'eyes', i.e., observations.\n",
    "\n",
    "Specifically we need to inform the agent of 2 more things:\n",
    "\n",
    "- The current position (our observation).\n",
    "- Whether we have reached the exit (are we done?).\n",
    "\n",
    "### To Do\n",
    "Implement the `create_observation_space`, `obtain_observation` and `is_done` methods.\n",
    "\n",
    "_Hint: `qgym` provides a ready-to-use `Discrete` action/observation space._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b9ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeState(MazeState):\n",
    "    def create_observation_space(self):\n",
    "        return Discrete(self.nrows * self.ncols)  # {0, 1,..., nrows x ncols-1}\n",
    "\n",
    "    def obtain_observation(self):\n",
    "        position = self.rowcol_to_pos(*self.position)\n",
    "        return position\n",
    "\n",
    "    def is_done(self):\n",
    "        row, col = self.position\n",
    "        return self.maze_map[row][col] == b\"G\"\n",
    "\n",
    "    def obtain_info(self):\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994b479e",
   "metadata": {},
   "source": [
    "# Setup the Environment\n",
    "The state space is now nicely defined.\n",
    "We can proceed with settings up the environment.\n",
    "The `Environment` is the glue that holds the state space, observation space and action space together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae7d57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze(Environment):\n",
    "    def __init__(self, maze_map):\n",
    "        self._state = MazeState(maze_map)\n",
    "        self.observation_space = self._state.create_observation_space()\n",
    "        self.action_space = self.action_space = Discrete(4)  # {0,1,2,3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4008e",
   "metadata": {},
   "source": [
    "### Carrot and Stick\n",
    "The final step towards completing our `Maze` environment is about giving feedback. We can give 3 types of feedback:\n",
    "- positive feedback (carrot)\n",
    "- negative feedback (stick)\n",
    "- neutral feedback\n",
    "\n",
    "In Reinforcement Learning, feedback is given by means of rewards. The choice of rewarder function has a lot of influence on the learning ability of the agent.\n",
    "\n",
    "Only providing rewards might lead to slow training and too much exploration. However, big penalties could make the agent skip exploration and stick to a safe, potentially non-optimal path.\n",
    "\n",
    "Below is room for two the implementation rewarders `CarrotOnly` (1) and `CarrotAndSticks` (2):\n",
    "\n",
    "1. Provides a positive reward only when the goal is reached. Does nothing otherwise\n",
    "2. Provides a positive reward when the goal is reached. Gives a negative reward (penalty) when the agent bumps into a wall. Otherwise, nothing.\n",
    "\n",
    "We have also including code blocks for training an agent on the `Maze` environment with either rewarder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57ce82a",
   "metadata": {},
   "source": [
    "### To Do\n",
    "#### Carrot only\n",
    "In the first block below we can implement the first rewarder:\n",
    "\n",
    "1. Provides a positive reward only when the goal is reached. Does nothing otherwise\n",
    "\n",
    "After defining the rewarder we can train an agent by running the second code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrotOnly(Rewarder):\n",
    "    def __init__(self):\n",
    "        self._reward_range = (0, 1)\n",
    "\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        row, col = new_state.position\n",
    "        if new_state.maze_map[row][col] == b\"G\":\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f1aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment and rewarder\n",
    "env = Maze(maze_map_4x4)\n",
    "env.rewarder = CarrotOnly()\n",
    "\n",
    "# ensure that we have implemented our environment correctly\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define and train our model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebe53dd",
   "metadata": {},
   "source": [
    "### To Do\n",
    "#### Carrot and sticks\n",
    "In the first block below we can implement the second rewarder\n",
    "\n",
    "2. Provides a positive reward when the goal is reached. Gives a negative reward (penalty) when the agent bumps into a wall. Otherwise, nothing.\n",
    "\n",
    "After defining the rewarder we can train an agent by running the second code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd2cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarrotAndSticks(Rewarder):\n",
    "    def __init__(self):\n",
    "        self._reward_range = (-1, 10)\n",
    "\n",
    "    def compute_reward(self, old_state, action, new_state):\n",
    "        row, col = new_state[\"position\"]\n",
    "\n",
    "        if new_state[\"position\"] == old_state[\"position\"]:\n",
    "            return -1\n",
    "        elif new_state[\"maze_map\"][row][col] == b\"G\":\n",
    "            return 10\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f66728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define environment and rewarder\n",
    "env = Maze(maze_map_4x4)\n",
    "env.rewarder = CarrotAndSticks()\n",
    "\n",
    "# ensure that we have implemented our environment correctly\n",
    "check_env(env, warn=True)\n",
    "\n",
    "# define and train our model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(int(1e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e076bc5-2d69-4567-a2a1-c0f171be2cab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
