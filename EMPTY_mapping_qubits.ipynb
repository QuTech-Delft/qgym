{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c032998e",
   "metadata": {},
   "source": [
    "# Mapping qubits\n",
    "In this notebook we will cover the QGym `InitialMapping` environment.\n",
    "\n",
    "This environment is aimed at solving the problem of mapping virtual to physical qubits that have a certain topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4505d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.generators import gnp_random_graph\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from qgym.envs.initial_mapping import InitialMapping\n",
    "from qgym.envs.initial_mapping.initial_mapping_rewarders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f9bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_rgb(step, rgb_array):\n",
    "    \"\"\"\n",
    "    Convenience method that we will use later on to display our results.\n",
    "    \"\"\"\n",
    "    clear_output(wait=True)\n",
    "    plt.figure(figsize=(40, 20))\n",
    "    plt.title(f\"Step {step}\", fontsize=40)\n",
    "    plt.imshow(rgb_array)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda37cd9",
   "metadata": {},
   "source": [
    "### Connection and interaction graph\n",
    "\n",
    "The initial mapping problem is focussed around two graphs:\n",
    "\n",
    "- connection graph: hardware layout describing the connections between physical qubits\n",
    "- interaction graph: software layout describing which virtual qubits interact in the particular quantum program\n",
    "\n",
    "The goal of the initial mapping problem is to find an optimal one-to-one between the virtual qubits of the interaction graph and the physical qubits of the connection graph.\n",
    "\n",
    "For now, we will consider an optimal mapping to be any mapping where the number of edges of the mapped interaction graph that do not coincide with edges of the connection graph is minimal.\n",
    "\n",
    "#### Toy hardware\n",
    "\n",
    "To explain this concept in more detail we start by defining a toy connection graph and by taking a look at some potential interaction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5551757b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(0, 2)\n",
    "connection_graph.add_edge(0, 3)\n",
    "nx.draw(connection_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1de923",
   "metadata": {},
   "source": [
    "Now let's take a look at some random interaction graphs, and think about how these can be best mapped on the connection graph.\n",
    "\n",
    "### To Do\n",
    "Implement the `generate_random_interaction_graph` in the code block below.\n",
    "\n",
    "_We can simply generate random graphs using [`gnp_random_graph`](https://networkx.org/documentation/stable/reference/generated/networkx.generators.random_graphs.gnp_random_graph.html)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747f38e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_interaction_graph(connection_graph):\n",
    "    # todo: implement this\n",
    "    \n",
    "interaction_graph = generate_random_interaction_graph(connection_graph)\n",
    "nx.draw(interaction_graph, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff55e3",
   "metadata": {},
   "source": [
    "### `InitialMapping` environment\n",
    "\n",
    "The `InitialMapping` environment can be initialized by providing:\n",
    "- a connection graph; and\n",
    "- an edge probability (i.e. the probability that an edge in the interaction graph exists)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa85a6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe270a85",
   "metadata": {},
   "source": [
    "#### State space\n",
    "The state space is described by a dictionary with the following structure:\n",
    "\n",
    "- `connection_graph_matrix`: A sparse adjacency matrix of the connection graph.\n",
    "- `num_nodes`: Number of physical qubits\n",
    "- `interaction_graph_matrix`: A sparse adjacency matrix of the interaction graph. _(Should have the same number of nodes as the connection graph.)_\n",
    "- `steps_done`: Number of steps done since the last reset.\n",
    "- `mapping`: A list of which the index represents a physical qubit and the value a virtual qubit (is set to `num_nodes` + 1 when nothing is mapped to the physical qubit yet).\n",
    "- `mapping_dict`: A dictionary that maps logical qubits to physical qubit.\n",
    "- `physical_qubits_mapped`. A set containing all physical qubits that have been mapped already.\n",
    "- `virtual_qubits_mapped`. A set containing all virtual qubits that have been mapped already.\n",
    "\n",
    "### To Do\n",
    "Take a look at the state space in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a2fe33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: take a look at the state space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9e0931",
   "metadata": {},
   "source": [
    "#### Action space\n",
    "A valid action is a tuple of integers $(i,j)$, such that $0 \\leq i,j < n$ where $n$ is the number of physical qubits. The action $(i,j)$ maps virtual qubit $j$ to phyiscal qubit $i$ when this action is legal. An action is legal when:\n",
    "1. virtual qubit $i$ has not been mapped to another physical qubit; and\n",
    "2. no other virual qubit has been mapped to physical qubit $j$.\n",
    "\n",
    "### To Do\n",
    "Take a look at the action space in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3077e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: take a look at the action space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72160a43",
   "metadata": {},
   "source": [
    "#### Observation space\n",
    "\n",
    "Each element in the observation space is a dictionary with 2 entries:\n",
    "- `mapping`: the current state of the mapping\n",
    "- `interaction_matrix`: the flattened adjacency matrix of the interaction graph\n",
    "\n",
    "### To Do\n",
    "Take a look at the observation space and see what the reset() method returns in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b441f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: take a look at the observation space and see what the reset() method returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452ae6a1",
   "metadata": {},
   "source": [
    "#### Rewarders\n",
    "\n",
    "We have pre-defined 3 different rewarders, all of which return a penalty when an illegal action is attempted, for a valid action their behaviour is as follows:\n",
    "\n",
    "- `BasicRewarder`: Reward is computed over all edges that have been mapped so far.\n",
    "- `SingleStepRewader`: Reward is computed over all new edges that have been mapped due to this action.\n",
    "- `EpisodeRewarder`: Only the final step results in a reward.\n",
    "\n",
    "All these rewarders can be tweaked by altering either of their parameters:\n",
    "\n",
    "- `illegal_action_penalty`: penalty given for attempting an illegal action (should be non-positive)\n",
    "- `reward_per_edge`: reward giving for correctly mapped edges (should be non-negative)\n",
    "- `penalty_per_edge`: penalty given for incorrectly mapped edges (should be non-positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f2f27",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Human Intelligence\n",
    "Let's attempt to determine the optimal mapping from our randomly generated interaction graph to our toy connection graph. Since this environment is still quite straightforward, we should be able to solve this case optimally by hand.\n",
    "\n",
    "Don't forget to take a look at the obtained rewards and observations after each step.\n",
    "\n",
    "_Note: It might be that multiple mappings our optimal._\n",
    "\n",
    "### To Do\n",
    "Pick your favorite rewarder in the first code block below.\n",
    "Next, try to solve the problem by giving the actions which you think are correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b75cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InitialMapping(0.5, connection_graph=connection_graph)\n",
    "\n",
    "# todo: pick your favorite rewarder\n",
    "# env.rewarder =\n",
    "\n",
    "obs = env.reset(interaction_graph=interaction_graph)\n",
    "print(f\"observation: {obs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931d8bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, rewards, done, info = env.step(# todo: fill in the correct action here)\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(1, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0781990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, rewards, done, info = env.step(# todo: fill in the correct action here)\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(2, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f421af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, rewards, done, info = env.step(# todo: fill in the correct action here)\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(3, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cfb274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obs, rewards, done, info = env.step(# todo: fill in the correct action here)\n",
    "print(f\"observation: {obs}\\n\\nreward: {rewards}\")\n",
    "render_rgb(4, env.render(mode=\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8adc50d2",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Reinforcement learning\n",
    "\n",
    "Can we achieve the same using reinforcement learning?\n",
    "\n",
    "Does changing the rewarder and/or its parameter give better results?\n",
    "\n",
    "### To Do\n",
    "Train a model on this environment in the first code block below.\n",
    "Next, run the second block to see how well the agent performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779042e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: train a model on this environment. (1e5 steps should be sufficient)\n",
    "model.save(\"initial_mapping_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56766025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(\"initial_mapping_1\")\n",
    "obs = env.reset(interaction_graph=connection_graph)\n",
    "for i in range(1000):\n",
    "    action, states = model.predict(obs, deterministic=False)\n",
    "    obs, rewards, done, info = env.step(action)\n",
    "    render_rgb(i, env.render(mode=\"rgb_array\"))\n",
    "    if done:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9198cf7c",
   "metadata": {},
   "source": [
    "Let's try another interaction graph.\n",
    "\n",
    "### To Do\n",
    "Design a nice interaction graph in the first code block.\n",
    "Next, run the second block to see if the agent can map it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aeb5b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: make a nice interaction graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd1a18b",
   "metadata": {},
   "source": [
    "Just to be sure, one more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ca20e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: make a nice interaction graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c42bc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b13d19f",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### More realistic hardware\n",
    "\n",
    "Having seen that we are able to train an agent on a toy environment, let's take a look at a more realistic hardware topology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510375b",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_graph = nx.Graph()\n",
    "connection_graph.add_edge(0, 1)\n",
    "connection_graph.add_edge(1, 2)\n",
    "connection_graph.add_edge(2, 0)\n",
    "connection_graph.add_edge(2, 3)\n",
    "connection_graph.add_edge(3, 4)\n",
    "connection_graph.add_edge(4, 2)\n",
    "nx.draw(connection_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf05a8",
   "metadata": {},
   "source": [
    "### To Do\n",
    "Use the first code block to create a new environment with the new connection graph, set a rewarder and train an agent.\n",
    "Use the second code block to design a interaction graph of your choice.\n",
    "Finally, use the third code block to see how well the agent performs on your interaction graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: train a model on this environment, which rewarder works best? (1e5 steps should be sufficient)\n",
    "model.save(\"initial_mapping_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0357a47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: make a nice interaction graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1caca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: let's test again (remember that you can load and save different models if you want)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef9c32a",
   "metadata": {},
   "source": [
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "### Connection fidelity\n",
    "\n",
    "Up to now, we have only used the `EpisodeRewarder`. However, we can also define rewarders differently in order to improve performance.\n",
    "\n",
    "Alternatively, we can define a different rewarder in order to change the objective. Since, for most quantum hardware not every connection has the same fidelity we might want to take this into account for the computation of our reward, such that our agent attempts to find a solution which not only requires a small amount of swap gates but also takes fidelity into account.\n",
    "\n",
    "Let's first define a weighted connection graph!\n",
    "\n",
    "### To Do\n",
    "Define a weighted connection graph in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81fa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: define a weighted connection graph\n",
    "\n",
    "# display graph with edge weights\n",
    "pos = nx.spring_layout(connection_graph, seed=0)\n",
    "edge_labels = nx.get_edge_attributes(connection_graph, \"weight\")\n",
    "nx.draw(connection_graph, pos, with_labels=True)\n",
    "nx.draw_networkx_edge_labels(connection_graph, pos, edge_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1de0f",
   "metadata": {},
   "source": [
    "And now we can implement a new rewarder.\n",
    "\n",
    "### To Do \n",
    "Implement the rest of this rewarder such that it takes the connection fidelity into account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbc41fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FidelityRewarder(BasicRewarder):\n",
    "    def compute_reward(\n",
    "        self,\n",
    "        *,\n",
    "        old_state: Dict[Any, Any],\n",
    "        action: NDArray[np.int_],\n",
    "        new_state: Dict[Any, Any],\n",
    "    ):\n",
    "        if self._is_illegal(action, old_state):\n",
    "            return self._illegal_action_penalty\n",
    "\n",
    "        # todo: implement the rest of this rewarder such that it takes the connection fidelity into account"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50865285",
   "metadata": {},
   "source": [
    "Time for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3e093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: train and don't forget to SAVE your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8963abbf",
   "metadata": {},
   "source": [
    "Does this rewarder do the job? Or, is some more tweaking required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: which interaction graph would be best to test if our agent has learned what to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd68c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: test the models behaviour"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
